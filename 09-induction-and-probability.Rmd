# Probability & Induction

We met some common forms of inductive argument back in [Chapter 2][Logic]. Now that we know how to work with probability, let's use what we know to better understand how one of those argument forms works.


## Generalizing from Observed Instances

One form of inductive argument we met generalizes from observed instances. For example, if you want to know what colour a particular species of bird tends to be, you might go out and look at a bunch of examples:

> Every raven I have ever seen has been black.\
> Therefore, all ravens are black.

How strong is this argument?

Observing ravens is a lot like sampling from an urn. Each raven is a marble, and the population of all ravens is the urn. We don't know what nature's "urn" contains at first: it might contain only black ravens, or it might contain ravens of other colours too. To assess the argument's strength, we have to calculate $\p(A \given B_1 \wedge B_2 \wedge \ldots \wedge B_n)$: the probability that all ravens in nature's urn are black, given that the first raven "drawn from nature's urn" was black, the second was too, and the third, and so on all the way up to the $n$^th^ raven observed. How do we calculate this kind of probability?

We already know how to solve such problems if there's just one observation. We use Bayes' theorem (or a probability tree). For example, imagine there are two urns:

- Urn X contains 100 black marbles.
- Urn Y contains 50 black marbles, 50 white.

The urns aren't labeled, and you pick one to sample from at random, so we'll assume the probability is $1/2$ that you're drawing from Urn X vs. Urn Y.

What is $\p(X \given B_1)$, the probability that you're drawing from the all-black urn given that the first draw is black? By Bayes' theorem:
$$
  \begin{aligned}
    \p(X \given B_1) &= \frac{\p(B_1 \given X)\p(X)}{\p(B_1 \given X) \p(X) + \p(B_1 \given \neg X) \p(\neg X)} \\
      &= \frac{(1/1)(1/2)}{(1/1)(1/2) + (1/2)(1/2)}\\
      &= 2/3.
  \end{aligned}
$$
So after one draw, the chance all the marbles are black goes up from $1/2$ to $2/3$.

You've probably seen more than just one raven in your life, though. So what happens when we keep drawing more marbles from the urn? How do we calculate $\p(A \given B_1 \wedge B_2)$, for example?


## Multiple Conditions

Suppose we put the first marble back, give the urn a good shake, and draw at random again. If the second draw comes up black as well, we need to calculate $\p(X \given B_1 \wedge B_2)$. If we apply Bayes' theorem now we get:
$$
  \begin{aligned}
    \p(X \given B_1 \wedge B_2) &= \frac{\p(B_1 \wedge B_2 \given X)\p(X)}{\p(B_1 \wedge B_2 \given X) \p(X) + \p(B_1 \wedge B_2 \given \neg X) \p(\neg X)}. \\ 
  \end{aligned}
$$
To fill in the values on the right hand side, we need to know these quantities:

- $\p(B_1 \wedge B_2 \given X)$, and
- $\p(B_1 \wedge B_2 \given \neg X)$.

The first one is easy because, if we're drawing from Urn X, the probability of getting a black marble is always $1$. That's the only kind of marble in Urn X. So:
$$\p(B_1 \wedge B_2 \given X) = 1.$$

The second quantity isn't much harder, though, because we replaced the first marble. That means, even if we're drawing from Urn Y, there are still 50 black marbles and 50 white on each draw. So if we're drawing from Urn Y, the draws are independent. In other words:
$$ \p(B_1 \wedge B_2 \given \neg X) = \p(B_1 \given \neg X) \p(B_2 \given \neg X).$$
In fact, the two probabilities on the right here are the same, since there's the same $1/2$ chance of getting a black marble from Urn Y on each draw. So
$$ \p(B_1 \wedge B_2 \given \neg X) = (1/2)(1/2) = 1/4.$$

Returning to Bayes' theorem to finish the calculation:
$$
  \begin{aligned}
    \p(X \given B_1 \wedge B_2) &= \frac{\p(B_1 \wedge B_2 \given X)\p(X)}{\p(B_1 \wedge B_2 \given X) \p(X) + \p(B_1 \wedge B_2 \given \neg X) \p(\neg X)} \\ 
    &= \frac{(1)(1/2)}{(1)(1/2) + (1/4)(1/2)}\\
    &= 4/5.
  \end{aligned}
$$
You might be able to guess now what will happen after three draws. A similar calculation will give us the result:
$$ \p(X \given B_1 \wedge B_2 \wedge B_3) = 8/9. $$
And after four black draws the probability will go up to $16/17$, and so on.

We could keep going, calculating $\p(A \given B_1 \wedge \ldots \wedge B_n)$ for any number $n$ you like, using Bayes' theorem.

But these calculations rely on putting the marble back with each draw. That's what makes the draws independent, given which urn we're drawing from. And that let's us "factor" the probabilities we need for Bayes' theorem:
$$
  \begin{aligned}
    \p(B_1 \wedge \ldots \wedge B_n \given \neg X) &= \p(B_1 \given \neg X) \times \ldots \times \p(B_n \given \neg X)\\
      &= (1/2)^n.
  \end{aligned}
$$
In real life though, we often sample without replacement. For example, pollsters don't call the same person twice to ask who they're voting for. And medical researchers only test a new drug on each subject once.


## Without Replacement

So suppose we don't replace the marble after the first draw. How do we calculate $\p(X \given B_1 \wedge B_2)$ then?

We're still going to start in the same place, with Bayes' theorem:
$$
  \begin{aligned}
    \p(X \given B_1 \wedge B_2) &= \frac{\p(B_1 \wedge B_2 \given X)\p(X)}{\p(B_1 \wedge B_2 \given X) \p(X) + \p(B_1 \wedge B_2 \given \neg X) \p(\neg X)}.
  \end{aligned}
$$
And we can still rely on the fact Urn X contains only black marbles to say:
$$ \p(B_1 \wedge B_2 \given X) = 1. $$
But what about the last term we need:
$$ \p(B_1 \wedge B_2 \given \neg X) = \;? $$
Let's think it through in two steps. We know the first draw has a $1/2$ chance of coming up black if we draw from Urn Y:
$$ \p(B_1 \given \neg X) = 1/2. $$
And once the first draw has come up black, assuming we're drawing from Urn Y, there are 49 black balls remaining and 50 white. So:
$$ \p(B_2 \given \neg X \wedge B_1) = 49/99. $$
So instead of multiplying one half by one half, we multiply one half by *almost* one half:
$$
  \begin{aligned}
    \p(B_1 \wedge B_2 \given \neg X) &= (1/2)(49/99)\\
       &= 49/198.
  \end{aligned}
$$

Returning to Bayes' theorem to finish the calculation:
$$
  \begin{aligned}
    \p(X \given B_1 \wedge B_2) &= \frac{\p(B_1 \wedge B_2 \given X)\p(X)}{\p(B_1 \wedge B_2 \given X) \p(X) + \p(B_1 \wedge B_2 \given \neg X) \p(\neg X)} \\
      &= \frac{(1)(1/2)}{(1)(1/2) + (49/198)(1/2)} \\
      &= 198/247 \\
      &\approx .802. 
  \end{aligned}
$$
Notice how similar this answer is to the $4/5$ we got when sampling with replacement. With so many black and white marbles in the urn, taking one out doesn't make much difference to the ratio of black-to-white marbles. So the second draw is almost the same as the first.


## Multiplying Conditional Probabilities

The calculation we just did relied on a new rule, which we should make explicit. Recall:

The General Multiplication Rule

:    $\p(A \wedge B) = \p(A \given B) \p(B).$

Our new rule just applies the same idea to situations where some proposition $C$ is taken as a given.

The General Multiplication Rule (Conditional Version)

:    $\p(A \wedge B \given C) = \p(A \given B \wedge C) \p(B \given C).$

In fact, this new rule follows from the familiar one. We just have to realize that the probabilities we get when we take a condition $C$ as given are still probabilities. They still follow all the same rules as unconditional probabilities. For example, the following rule is also valid:

The Negation Rule (Conditional Version)

:    $\p(\neg A \given C) = 1 - \p(A \given C).$

We could go through all the rules of probability we've learned and write out the conditional version for each one. But we've already got enough rules and equations to keep track of. So let's just remember this mantra instead:

```{block, type='info'}
Conditional probabilities are probabilities.
```

So if we have a rule of probability, the same rule will hold if we add a condition $C$ into each of the $\p(\ldots)$ terms.


## Multiple Witnesses

Let's do one more problem where we have to calculate a probability given multiple conditions. Recall the taxicab problem from [Chapter 8][Bayes' Theorem].

```{block, type='problem'}
A cab was involved in a hit and run accident at night. Two cab companies, the Green and the Blue, operate in the city. You are given the following data:

1.  85% of the cabs in the city are Green and 15% are Blue.
2.  A witness identified the cab as Blue. The court tested the reliability of the witness under the same circumstances that existed on the night of the accident and concluded that the witness correctly identified each one of the two colors 80% of the time and failed 20% of the time.

What is the probability that the cab involved in the accident was blue rather green?
```

We saw it's only about $41\%$ likely the cab was really blue, even with the witness' testimony. But what if there had been two witnesses, both saying the cab was blue?

Let's use Bayes' theorem again:
$$
  \begin{aligned}
    \p(B \given W_1 \wedge W_2) &= \frac{\p(B)\p(W_1 \wedge W_2 \given B)}{\p(W_1 \wedge W_2)}.
  \end{aligned}
$$
We have one of the terms here already: $\p(B) = 15/100$. What about the other two terms?

- $\p(W_1 \wedge W_2 \given B) = \;?$,
- $\p(W_1 \wedge W_2) = \;?$.

Let's make things easy on ourselves by assuming our two witnesses are reporting independently. They don't talk to each other, or influence one another in any way. They're only reporting what they saw (or think they saw). Then we can "factor" these probabilities like we did when sampling with replacement:
$$
  \begin{aligned}
    \p(W_1 \wedge W_2 \given B) &= \p(W_1 \given B) \p(W_2 \given B)\\
                                &= (80/100)(80/100).
  \end{aligned}
$$
And for the denominator we use the Law of Total Probability:
$$
  \begin{aligned}
    \p(W_1 \wedge W_2) &= \p(W_1 \wedge W_2 \given B)\p(B) + 
                          \p(W_1 \wedge W_2 \given \neg B)\p(\neg B)\\
                       &= (80/100)(80/100)(15/100) + (20/100)(20/100)(85/100)\\
                       &= 96/1000 + 34/1000\\
                       &= 13/100.
  \end{aligned}
$$

Now we can return to Bayes' theorem to finish the problem:
$$
  \begin{aligned}
    \p(B \given W_1 \wedge W_2) &= \frac{(15/100)(80/100)(80/100)}{13/100}\\
                                &= 96/130\\
                                &\approx .74.
  \end{aligned}
$$
So, with two witnesses independently agreeing that the cab was blue, the probability goes up from less than $1/2$ to almost $3/4$.


## A Pause for Breath

We started this chapter with a question. How do we assess the strength of an argument that generalizes from observed instances? To answer that question, we drew an anlogy with a familiar problem we already have the tools to solve: determining the contents of an urn by sampling.

This analogy led us to develop two strategies for calculating conditional probabilities, when more than one condition is given.

`r newthought("The first strategy")` is easier. But it only works when the conditions are appropriately independent. Like when we sample with replacement, or when two witnesses independently report what they saw.

In this case we first use Bayes' theorem, then "factor" the terms:
$$
  \begin{aligned}
    \p(A \given B_1 \wedge B_2) &= 
      \frac{\p(B_1 \wedge B_2 \given A)\p(A)}{%
            \p(B_1 \wedge B_2 \given A)\p(A) +%
              \p(B_1 \wedge B_2 \given \neg A)\p(\neg A)}\\
      &= \frac{\p(B_1 \given A)\p(B_2 \given A)\p(A)}{%
                \p(B_1 \given A)\p(B_2 \given A)\p(A) +%
                  \p(B_1 \given \neg A)\p(B_2 \given \neg A)\p(\neg A)}\\
      &= \frac{[\p(B_1 \given A)]^2\p(A)}{%
                [\p(B_1 \given A)]^2\p(A) +%
                  [\p(B_1 \given \neg A)]^2\p(\neg A)}.
  \end{aligned}
$$

`r newthought("The second strategy")` is a little more difficult. But it works even when the conditions aren't independent. In this case we still start with Bayes' theorem, but then we apply the conditional form of the General Multiplication Rule:
$$
  \begin{aligned}
    \p(A \given B_1 \wedge B_2) &= 
      \frac{\p(B_1 \wedge B_2 \given A)\p(A)}{%
            \p(B_1 \wedge B_2 \given A)\p(A) +%
              \p(B_1 \wedge B_2 \given \neg A)\p(\neg A)}\\
      &= \frac{\p(B_2 \given B_1 \wedge A)\p(B_1 \given A)\p(A)}{%
                \p(B_2 \given B_1 \wedge A)\p(B_1 \given A)\p(A) +%
                  \p(B_2 \given B_1 \wedge \neg A)\p(B_1 \given \neg A)\p(\neg A)}.
  \end{aligned}
$$ 

`r newthought("We've made a lot of progress.")` We've significantly sharpened our understanding of inductive reasoning. And we've developed precise techniques for evaluating actual inductive arguments.

But we haven't completely answered the question we started out with. And it's important to understand why not.


## Real Life Is More Complicated 

There are two major limitations to our analogy between sampling marbles and  observing ravens.

`r newthought("The first limitation")` is that the ravens we observe in real life aren't randomly sampled from nature's "urn". We only observe ravens in certain locations, for example. But our solution to the urn problem relies crucially on random sampling. For example, we assumed $\p(B_1 \given \neg X) = 1/2$ because each of the 50 black marbles in Urn Y is just as likely as the each of the 50 white marbles to be selected.

If there are white ravens in the world, they might be limited to some far away location. So the fact we're only observing ravens in our part of the world could make a big difference to what we find. It matters whether your sample really is random.

`r newthought("The second limitation")` is that we pretended there were only two possibilities: either all the marbles in the urn are black, or half of them are. And, so, we assumed there was already a $50\%$ chance all the marbles are black, before we even looked at any of them.

In real life though, when we encounter a new species, it could be $90\%$ of them are black, or $31\%$, or $42.718\%$, or any number between $0\%$ and $100\%$. So there are many, many more possibilities. The possibility that all $100\%$ are black is just one of these. So it would start with a much lower probability than $1/2$.

We could make our analysis more realistic by taking all these complications into account. But the calculations would get very ugly, and we'd have to use calculus to solve the problem. This is the kind of technical topic you'd cover in a math or statistics class on probability. But it's not the kind thing this book is about.

We just want to understand the key ideas:

- The strength of an inductive argument is a question of conditional probability. How probable is the argument's conclusion given its premises?

- An argument that generalizes from observed instances is similar to an urn problem, where we guess the contents of the urn by repeated sampling.

- These urn problems involve calculating conditional probabilities given multiple conditions. They can be solved with Bayes' theorem.

- Using Bayes' theorem with multiple conditions is easiest when the sampling is done with replacement. Then we can use independence to "factor" some the quantities we need. 

    Otherwise, we need the conditional form of the General Multiplication Rule.

