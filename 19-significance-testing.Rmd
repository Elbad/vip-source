# Significance Testing

How do we use evidence to evaluate hypothesis according to frequentism? The short answer is, we look for things that would be too much of a coincidence if the hypothesis were true.


## Coincidence

Suppose we flip a coin ten times and it lands heads every single time. That would be too much of a coincidence if the coin were fair. So the hypothesis that it is fair has been tested, and failed. Conclusion: the coin is biased towards heads.

Or imagine we divide a thousand patients with Disease X into two, equal-sized groups. We give the first group Drug Y, the second group gets a placebo. After a month, $90\%$ of the patients who got the drug are cured, compared to only $10\%$ of the patients who didn't. That would be too much of a coincidence if the drug were ineffective. So the hypothesis that the drug has no effect has been tested, and it has failed the test. Conclusion: the drug helps cure Disease X.

That's the rough idea behind the most popular frequentist approach to hypothesis-testing. Now let's take a closer look.


## Normal Approximations

Suppose your friend likes to do magic tricks. Their favourite trick involves flipping their favourite "magic" coin and predicting how it will land. You're curious how the trick is done, and you suspect the coin isn't fair. So you flip it $100$ times to see what happens. It lands heads $67$ out of $100$ times. Does that mean the coin isn't fair after all? Or is $67$ heads out of $100$ flips within the realm of plausibility for a fair coin?

We need to figure out just how unlikely it is to get $67$ or more heads. This would be tedious to do with precision, but there's a trick for getting a rough answer.

If you flip a fair coin $100$ times, the most likely outcome is $50$ heads. A bit less likely is $51$ heads, or $49$ heads. Still less likely is $52$ heads, or $48$ heads. And so on. The overall pattern looks like this:

```{r echo=FALSE, fig.cap="Lorem ipsum"}
df <- data.frame(heads = 0:100, probability = dbinom(0:100, 100, 0.5))

ggplot(df, aes(x = heads, y = probability)) + 
  geom_bar(stat = "identity", fill = "firebrick")
```

```{r echo=FALSE, fig.margin=TRUE, fig.cap="The famous normal distribution, sometimes called the \"bell curve\""}
ggplot(data.frame(x = c(0, 100))) + 
  stat_function(aes(x), fun = dnorm, args = list(mean = 50, sd = 5), n = 201, colour = "steelblue", size = .75) +
  theme_void()
```

Notice how much it resembles the famous "bell curve" shape. In fact they line up almost perfectly:

```{r echo=FALSE, fig.cap="Lorem ipsum"}
ggplot(df, aes(x = heads, y = probability)) + 
  geom_bar(stat = "identity", fill = "firebrick") +
  stat_function(aes(heads), fun = dnorm, args = list(mean = 50, sd = 5), n = 201, colour = "steelblue", size = .75)
```

The bell curve's formal name is the ***normal distribution***, and it has some very handy mathematical features. We'll get to the deails in a moment, but basically, some simple calculations can tell us where the normal distribution starts to flatten out to zero on each side. Which makes it easy to estimate when the number of heads is too high (or too low) to be within the realm of plausibility.

```{marginfigure, echo=TRUE}
The normal distribution itself actually has a very complicated mathematical formula. You *really* don't need to know it for this book. But for those who are curious, here it is:
$$ f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x - \mu)^2}{2\sigma^2}} .$$
```

Two features of the normal distribution are required for the calculation.

```{marginfigure, echo=TRUE}
The symbol $\mu$ is from Greek and is called *mu* (rhymes with *stew*). It looks like the English letter *u*, but it actually corresponds to the letter *m*, as in *m*ean. It helps to picture it as a cursive *m*, written very sloppily by someone in a hurry.
```

First, where is the bell located, what's its center? This is called the *mean*, or $\mu$ for short. In our example $\mu = 50$. That's the most likely outcome for $100$ flips of a fair coin. The general formula is
$$ \mu = np, $$
where $n$ is the number of tosses, and $p$ is the probability of heads on each toss. So in our example $np = (100)(1/2) = 50$.

```{r echo=FALSE, fig.margin=TRUE, fig.cap="Bell curves with various standard deviations, making them wider or thinner"}
knitr::include_graphics("img/marg_fig.png")
```

```{marginfigure, echo=TRUE}
The Greek symbol $\sigma$ is called *sigma*. It corresponds to the English letter *s*, as in *s*tandard deviation.
```

Second, how wide is the bell? This is called the *standard deviation*, or $\sigma$ for short. The formula for $\sigma$ is a bit mysterious:
$$ \sigma = \sqrt{np(1-p)}. $$
The derivation of this formula is pretty advanced, so we'll just take it on faith here. Applying the formula to our example we get:
$$
  \begin{aligned}
    \sigma &= \sqrt{(100)(1/2)(1/2)}\\
           &= 5.
  \end{aligned}
$$

Now for the punchline: we can use the numbers $\mu$ and $\sigma$ to get a pretty accurate estimate of how probable different outcomes are. Mathematicians have proved that about $99\%$ of the time, the number of heads will be within the range $\mu \pm 3\sigma$. In our example $3\sigma = (3)(5) = 15$, so $\mu \pm 3\sigma$ is the range from $35$ to $65$.

We got 67 heads, which falls outside the $35$-to-$65$ range. Which means it's a pretty far out possibility! It's the kind of thing that should happen less than $1\%$ of the time, if the coin is fair.

So the hypothesis that your friend's coin is fair has failed our test. If it were fair, you'd expect something closer to $50$ heads out of $100$ flips.

That's the idea behind the frequentist method for testing a hypothesis. We compare what the hypothesis says should happen to what actually happens in our experiment. We can use the bell curve formulas to figure out whether what actually happens is within the realm of plausibility, if the hypothesis is correct. If the number of heads we actually get falls outside the $99\%$ range, for example, we reject our starting hypothesis. We conclude the coin is *not* fair.


## The 68-95-99 Rule

You don't have to set the cutoff for plausibility at $99\%$. In fact there are three cutoffs that are easy to calculate using $\mu$ and $\sigma$:

- There's about a $68\%$ chance the number of heads will be in the range $\mu \pm \sigma$.
- There's about a $95\%$ chance the number of heads will be in the range $\mu \pm 2\sigma$.
- There's about a $99\%$ chance the number of heads will be in the range $\mu \pm 3\sigma$.

In terms of a diagram:

```{r echo=FALSE, fig.cap="The 68-95-99 rule"}
knitr::include_graphics("img/fig.png")
```

To put the same idea in terms of probability statements, let's call the number of heads we actually get $k$. Then:
$$
  \begin{aligned}
    \p(\mu - \sigma \leq k \leq \mu + \sigma) &\approx .68,\\
    \p(\mu - 2\sigma \leq k \leq \mu + 2\sigma) &\approx .95,\\
    \p(\mu - 3\sigma \leq k \leq \mu + 3\sigma) &\approx .99.
  \end{aligned}
$$ 

This is called *the 68-95-99 rule*. We use it to get a sense of how surprising a result is. The farther $k$ is from the mean $\mu$, the more of a coincidence the result is, assuming the hypothesis we're testing is true.


## Binomial Probabilities

But when do probabilities match the bell curve? A lot of the time, actually. But we'll just focus on one common case.

Suppose a certain kind of event will be repeated over and over, and there are two possible outcomes. If the probabilities of these two outcomes stay the same from repetition to repetition, and the repetitions are independent of one another, then the probabilities are called *binomial*.

Binomial probabilities are very common. We saw that they apply to flips of a fair coin. Another example would be patients in a drug study who either get better or don't. Or subjects in a psychology study answering yes/no questions on a survey. Or respondents to a poll about a political race.

If the event is repeated enough times, the binomial probabilities will closely match the bell curve. So if you do a big enough study, you can use the 68-95-99 rule as a reliable estimate.

This is called a ***normal approximation***. We use the smooth bell curve of the normal distribution (in blue) to approximate the exact binomial probabilities (red).


## Significance Testing

This method of evaluating hypotheses is called *significance testing*. We want to know whether the number of heads we observed is "significant", whether it tells us something about the coin being biased.

Here is the general recipe for significance testing with binomial probabilities:

1. State the hypothesis you want to test: the true probability of outcome X is $p$.
2. Repeat the event over and over $n$ times, and count the number of times $k$ you get outcome X.
3. Calculate $\mu$ and $\sigma$ from $n$ and $p$.
3. Use the 68-95-99 rule to figure out how much of a coincidence your finding $k$ would be if the hypothesis is true.
4. If it's too much of a coincidence, conclude that the hypothesis is false. The true probability isn't $p$.

Let's do another example:

```{marginfigure, echo=TRUE}
The VisioPerfect example is from page 205 of Ian Hacking's excellent textbook, *An Introduction to Probability & Inductive Logic*.
```

```{block, type="example"}
A company named VisioPerfect makes "long-life" lightbulbs. According to their ads, $96\%$ of their bulbs outlast their competitors' average lifespan.

The magazine *Consumers' Advocate* decides to run a test of 2,400 VisioPerfect bulbs. They find that $133$ of them weren't "long-life".\
```

Is the result of this test bad news for VisioPerfect? Are their ads just hype? Let's test their claim.

The hypothesis we're testing is that each bulb has probability $.96$ of being long-life, which means it has probability $p = .04$ of being "short-life". The magazine found $k = 133$ short-life bulbs out of $n = 2,400$. Is this about what you'd expect if the ads are honest? Let's start by finding the center and width of our normal approximation:
$$
  \begin{aligned}
    \mu &= np\\
        &= (2,400)(.04)\\
        &= 96,\\
    \sigma &= \sqrt{np(1-p)}\\
           &= \sqrt{(2,400)(.04)(.96)}\\
           &= 9.6.  
  \end{aligned}
$$
Then we use the 68-95-99 rule to figure out where $k = 133$ falls. In this case the $99\%$ range is $96 \pm (3)(9.6)$, which is $67.2$ to $124.8$. Since $k = 133$ falls outside this range, it's a pretty far out result. It's the kind of thing you'd expect to happen less than $1\%$ of the time if each bulb really had a $96\%$ chance of being long-life.


## Significance Levels

What if the results had been different, like if they'd only found $k = 120$ short-life bulbs? That's still a pretty far out result. It's inside the $99\%$ range, but still outside the $95\%$ range. So it's the kind of thing you'd expect to happen less than $95\%$ of the time.

Sometimes when scientists report their results, they describe them as "significant at the .05 level", or "significant at the .01 level". That's because there's no precise meaning to "within the realm of plausibility". We could interpret that to mean "within the $99\%$ range", or we could understand it as "within the 95\% range" instead. There's nothing special or magical about the $95\%$ and $99\%$ cutoffs.

Why do scientists use the $95\%$ and $99\%$ cutoffs then? It's actually just a kind of historical and mathematical accident. Before computers, it was hard to calculate significance levels exactly. But calculating $\mu$ and $\sigma$ by hand was easy, so it was easy to calculate the ranges $\mu \pm 2\sigma$ and $\mu \pm 3\sigma$. Since these ranges correspond approximately to $95\%$ and $99\%$, scientists adopted those cutoffs.

Different sciences have different conventions about where to put the cutoff. Social sciences like Psychology typically use the $95\%$ cutoff. Only when the outcome of a study is significant at the $.05$ level is the hypothesis rejected. But medical and physical sciences often use a stricter cutoff like $99\%$. A finding has to be significant at the $.01$ level to disprove a hypothesis then.


## Warnings

```{marginfigure, echo=TRUE}
The American Statistical Association [recently released a statement](https://www.amstat.org/newsroom/pressreleases/P-ValueStatement.pdf) to clarify the ideas behind significance testing, and prevent their misuse. It's also the punchline of [one of my favourite cartoons](https://imgs.xkcd.com/comics/significant.png).
```

Significance testing is very, *very* confusing. Scientists, and even statisticians, often misunderstand and misapply it. Dangerous medical treatments have been approved and used as a result. Scientific careers have even been built on research that misuses significance testing.

So what exactly does it mean for a result to be "significant at level $\alpha$"?

```{marginfigure, echo=TRUE}
The Greek letter $\alpha$ is called *alpha* and corresponds to the English letter *a*. It's customarily used for significance levels, I don't know why.
```

```{block, type="warning"}
*If* the hypothesis we are testing is true, *then* a result this unusual was less than $\alpha$ likely to occur.
```
Notice how this is an *if/then* statement, where the *if* part supposes the hypothesis we're testing is true. We're considering what things look like from the hypothetical point of view where the hypothesis is true. Then we evaluate how probable various outcomes are, given that assumption.

But we really want to know the reverse thing. We want to know: *if* we get $k$ heads, *then* how likely is it the hypothesis is true? You might think if the result is significant at the $.05$ level, then the hypothesis is less than $.05$ probable. Not so!

```{block, type="warning"}
Just because the outcome of an experiment is significant at the $\alpha$ level doesn't mean the probability of the hypothesis is $\alpha$ (or less than $\alpha$).
```

We learned back in [Chapter 8][Bayes' Theorem] about how $\p(H \given E)$ and $\p(E \given H)$ are different things. In fact they can be very different; one can be high and the other low. That happens in the taxicab problem for example, where people tend to get them mixed up. It's a very common mistake to have a similar confusion with significance testing.



## Exercises {-}

#. Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. 

#. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.